{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSdF40XlqI1tehPDhirNfK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Correlation Coefficient r\n",
        "\n"
      ],
      "metadata": {
        "id": "mVTaGGrBr-PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W3qVkzd2pU2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Linear Regression\n",
        "\n",
        "Regression models are a class of statistical models that allows us to test the hypothesis of whether there is a significant relationship between a response variable and some explanatory variable(s). \n",
        "\n",
        "That is, given some explanatory variables, you can make predictions about the value of the response variable. \n",
        "\n",
        "The **response variable**, the one we want to make predictions on, is also known as the dependent variable or the variable Y. \n",
        "\n",
        "The **explanatory variables** are used to explain how the predictions will change, are also known as independent variables or variable X. \n",
        "\n",
        "Linear regression is used **when the response variable is continuous (i.e., numeric).**\n"
      ],
      "metadata": {
        "id": "e4uanbCA1xP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A regression is a straight line and the line is defined by two properties: \n",
        "\n",
        "**The intercept** is the y value, when x is zero. \n",
        "\n",
        "**The slope** is the steepness of the line, equal to the amount y increases if x is increased one unit at a time. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**The equation** for a straight line is that the y value is the intercept plus the slope times the x value.\n",
        "\n",
        "$$Y = intecept + slope * x$$\n",
        "\n",
        "Writen with the following formula for a hypothesized line:\n",
        "\n",
        "$$Y =  \\beta_0 + \\beta_1x$$ \n",
        "\n",
        "$\\beta_0$ is a constant\n",
        "\n",
        "$\\beta_1$ is the slope\n",
        "\n",
        "$x$ is the value of independent variable (the predictor)\n",
        "\n",
        "$Y$ is the value of the dependent variable (the response variable)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Estimating the intercept**. Where the trend line intercept the y axis.\n",
        "\n",
        "**Estimating the slope**. Calculate the change in y between two points (we calculate the change in x between two points and devide one by the other."
      ],
      "metadata": {
        "id": "ZMJk7vg5Ib9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. State the hypothesis**\n",
        "\n",
        "$H_0: b_1 = 0$\n",
        "\n",
        "$H_a: b_1 \\neq 0$\n",
        "\n",
        "Where $b_1$ is the slope of the regression line\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mJEq7S02Rquz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Checking conditions for inference**\n",
        "\n",
        "\n",
        "\n",
        "*   The dependent variable $Y$ has a linear relationship to the independent variable $X$.\n",
        "*   Any given x, y is independent\n",
        "*   y values are approx. normally distributed\n",
        "*   Check for homoscendasticity: For each value of x, the probability distribution of Y has the same variance ($\\sigma$)\n",
        "*   Check for multicollinearity: predictor variables are independent of each other \n",
        "*   Check for autocorrelation: error terms are independent (uncorrelated)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fVUgRDcAcrqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. State the significance level**"
      ],
      "metadata": {
        "id": "wCi2AWMhkyvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Calculate the t-test statistic**\n",
        "\n",
        "We first need to calculate the standard error of the slope.\n",
        "\n",
        "\n",
        "$s_{e}^2 =\\frac {\\sum{(y_i -\\bar{y_i})}^2} {n-k}$\n",
        "\n",
        "$\\bar{y_i}$ is the predicted value\n",
        "\n",
        "$y_i$ is the observed value\n",
        "\n",
        "${n-k}$ is the degrees of freedom\n",
        "\n",
        "**Example:** \n",
        "\n",
        " **Need to do more work here...**\n",
        "\n",
        "\n",
        " $t_c =\\frac {{b_1 -\\beta_1}^2} {S_b1}$\n",
        "\n",
        " $s_b1$ = standard deviation of the estimate of $b_1$\n",
        "\n",
        " The formula for $S_{b1}$:\n",
        "\n",
        " $S_{b1} = \\frac {{S_e}^2} {\\sqrt{(x_i-\\bar{x})^2}}$\n",
        "\n",
        " Degrees of freedom:\n",
        "\n",
        " $df = n-k -1$  \n",
        " \n",
        "Where $k$ is the number of independent variables and the extra $1$ is subtracted because of the intercept.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vURQxPJ0mjn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. **\n",
        "\n"
      ],
      "metadata": {
        "id": "p9FS-4poFKmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks of the day:**\n",
        "\n",
        "1.   We'll visualize and fit linear regressions\n",
        "2.   We'll make predictions with them\n",
        "3. We'll determine whether the model is a good fit\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rMp-uG1R2zyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different packages are used to run regression models. Statsmodes is a popular choice. \n",
        "\n",
        "**1a. Visualizing linear regressions**"
      ],
      "metadata": {
        "id": "tvBW1h344R6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing regressions\n",
        "\n",
        "import seaborn as sns\n",
        "import panda as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Draw the scatter plot\n",
        "sns.scatterplot(x = 'predictor_variable', y='response_variable', data = dataframe)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mdOGa_fPHT2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In seaborn,  the **regplot() function** adds a trend line calculated using linear regression. Also, by default, regplot() adds a confidence interval around the line, this can be helpful, but can also removed by setting the $ci$ argument to \"None\", in case, like now, when we're mostly interested in the reg line. "
      ],
      "metadata": {
        "id": "uiAvphMT2puD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw a trend line on the scatter plot \n",
        "sns.regplot(x=\"predictor_variable\",\n",
        "            y=\"response_variable\",\n",
        "            data=dataframe,\n",
        "            ci=None,\n",
        "            scatter_kws={'alpha': 0.5})\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ro8sZV7qxiUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1b. Fitting linear regressions:**\n",
        "\n",
        "**The function**\n",
        "\n",
        "The python function ols() takes two arguments. \n",
        "\n",
        "\n",
        "\n",
        "*   The first argument is a formula: the response variable is written to the left of the tilde, and the explanatory variable is written to the right. \n",
        "*   The data argument takes the DataFrame containing the variables.\n",
        "*   To actually fit the model, we add .fit()  to the created model object. \n",
        "*   Print the resulting model, using the params attribute.\n",
        "\n",
        "\n",
        "This will result in two coefficients. \n",
        "\n",
        "These coefficients are the intercept and slope of the straight line. \n",
        "\n",
        "How do we interpret these?\n",
        "\n",
        "For every additional x-value, we expect the total y to increase by what factor.\n"
      ],
      "metadata": {
        "id": "LLDyhJgCJuUH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pka-KuZ81TYx"
      },
      "outputs": [],
      "source": [
        "#Running a regression with continuous predictor\n",
        "\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "regression = ols(\"response_variable ~ predictor\", data = DataFrame)\n",
        "\n",
        "#Fitting a regression\n",
        "regression = regression.fit()\n",
        "\n",
        "# Print the parameters of the fitted model\n",
        "print(regression.params)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a regression with categorical predictor\n",
        "\n",
        "# Visualizing the data\n",
        "#we draw a histogram for each of the \"categories\". \n",
        "#Displot creates histograms.\n",
        "#Displot takes a DataFrame as the data argument: the variable of interest as x, and the variable used to split on as col.\n",
        "#To give a separate panel to each type,an optional col_wrap argument can be used (= number of plots per row). \n",
        "#It can be helpful to set the bins argument, how many bins are appropriate for this dataset? \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.displot(data = DataFrame, x = \"cat_predictor\", col = \"type\", col_wrap = 2, bins = 5)\n",
        "plt.show()\n",
        "\n",
        "#group categorical variable and calculate means\n",
        "\n",
        "summary_stats = dataframe.groupby(\"cat_predictor\")[\"response\"].mean()\n",
        "print(summary_stats)\n",
        "\n",
        "#Linear regression using \"response\" as response variable\n",
        "#\"predictor\" as the explanatory variable\n",
        "\n",
        "From statsmodels.formula.api import ols\n",
        "cat_reg_model = ols(\"response ~ cat_predictor\", data = dataframe).fit()\n",
        "print(cat_reg_model.params)\n",
        "\n",
        "#A coefficient for a level in the categorical variable is missing, but the number for the intercept looks familiar. \n",
        "#The intercept is the mean of one the predictor levels, we just calculated. \n",
        "#But what are the other coefficients then, how do their coefficient relate? \n",
        "#If it does not make sense, fortunately, we can fix it.\n",
        "\n",
        "#By changing the formula slightly to append \"plus zero\", we specify that all the coefficients should be given relative to zero. \n",
        "#This means we are fitting a linear regression without an intercept term.\n",
        "#however, in case of a single categorical variable, the coefficients are simply the means.\n",
        "\n",
        "reg_model_zero = ols(\"response ~ cat_predictor + 0\", data = datframe).fit()\n",
        "print(reg_model_zero.params)\n",
        "\n"
      ],
      "metadata": {
        "id": "MvxDr16UNvIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Making predictions**\n",
        "\n"
      ],
      "metadata": {
        "id": "EYwEOPRHlLgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call ols with (1) a formula and (2) the dataset, after which we add .fit(). \n",
        "\n",
        "The response variable goes on the left-hand side of the formula, and the explanatory variable goes on the right. \n",
        "\n",
        "We need to assign the result to a variable, so we can reuse it later on. \n",
        "\n",
        "To view the coefficients of the model, we can use the params attribute in a print call (like above).\n",
        "\n"
      ],
      "metadata": {
        "id": "xv8pVRcToCDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model = ols(\"response ~ predict\", data = dataframe).fit()\n",
        "print(reg_model.params)"
      ],
      "metadata": {
        "id": "sFBjDFWJoR0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to choose some values for the explanatory variables. To create new explanatory data, we store our explanatory variables in a new pandas DataFrame. The goal is to create a DataFrame that contains both the explanatory variable and the predicted response. \n",
        "\n",
        "Now this can allow us to answer questions that go beyond what the original dataset allows us to do (= make predictions). \n"
      ],
      "metadata": {
        "id": "RJaFSA8DowHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   We specify an interval of values using the np.arange function, taking the start and end of the interval as arguments. Note that the end of the interval, does not include this value.\n",
        "*   Then, call predict() on the model, passing the dataframe with the explanatory variables as the argument.\n",
        "*   The predict function returns a series of predictions, one for each row of the explanatory data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z7gOexj9qK-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Create explanatory_data \n",
        "explanatory_data = pd.DataFrame({'predictor': np.arange(0, 11)}) #values 1-10\n",
        "\n",
        "# Use \"model\" to predict with explanatory_data, call it response_var\n",
        "response_var = model.predict(explanatory_data)\n",
        "\n",
        "#we start with the existing column, explanatory_data. \n",
        "#Then, we use the pandas .assign() to add a new column, named after the response variable, response. \n",
        "\n",
        "# Create prediction_data\n",
        "prediction_data = explanatory_data.assign(response_var = response_var)\n",
        "\n",
        "# Print the result\n",
        "print(prediction_data)"
      ],
      "metadata": {
        "id": "ldPsZOeXzjJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with model objects, what are they?\n",
        "\n",
        "**.fittedvalues attribute** - \"Fitted vallues\" are predictions on the original dataset used to create the model.\n",
        "\n",
        "**.resid attribute** - are a measure of inaccuracy in the model fit. Like fitted values, there is one residual for each row of the dataset. Each residual is the actual response value minus the predicted response value.\n",
        "\n",
        "**.summary()**"
      ],
      "metadata": {
        "id": "Az8eDDQ0GqrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.params)\n",
        "print(model.fittedvalues) #equal to print(model.predict(explanatory_data))\n",
        "print(model.resid) \n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3EW74FXnI1qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients of mdl_price_vs_conv\n",
        "coeffs = model.params\n",
        "\n",
        "# Get the intercept\n",
        "intercept = coeffs[0]\n",
        "\n",
        "# Get the slope\n",
        "slope = coeffs[1]\n",
        "\n",
        "# Manually calculate the predictions\n",
        "reg = intercept + slope * explanatory_data\n",
        "print(reg)\n",
        "\n",
        "# Compare to the results from .predict()\n",
        "print(reg.assign(predictions_auto=reg_model.predict(explanatory_data)))"
      ],
      "metadata": {
        "id": "zRLVvc_6ODiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Assessing model fit**\n",
        "\n",
        "How good is our model?\n",
        "\n",
        "The **coefficient of determination**, also called r-squared. Either written with a lower case r for simple linear regression and an upper case R when there are several explanatory variables. \n",
        "\n",
        "$ r^2$ is defined as the proportion of the variance in the response variable that is predictable from the explanatory variable. \n",
        "\n",
        "A score of $1$ means the model has a perfect fit, and a score of $0$ means the model is no better than randomness. \n",
        "\n",
        "For simple linear regression, the interpretation of the coefficient of determination is straightforward. It is simply the correlation between the explanatory and response variables, squared. ( .rsquared attribute in python)\n",
        "\n",
        "What is a good $ r^2$? It depends on what your studying."
      ],
      "metadata": {
        "id": "Swuj7dVGRklx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation squared\n",
        "coeff_determination = variable[\"x\"].corr(variable[\"y\"]) ** 2\n",
        "print(coeff_determination)"
      ],
      "metadata": {
        "id": "rjRbLBezZhR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model = ols(\"y ~ x\", data = dataframe).fit()\n",
        "print(reg_model.summary())\n",
        "print(reg_model.rsquared)"
      ],
      "metadata": {
        "id": "-3h5lr6aMzl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to assess model fit is through **the residual standard error, or RSE**. RSE tells us how much predictions are typically wrong.\n",
        "\n",
        "Specifically, each residual is the difference between a predicted value and an observed value. \n",
        "\n",
        "The **mean squared error, or MSE**, is the squared residual standard error. We can calculate the RSE by taking the square root of MSE. \n",
        "\n",
        "$MSE = RSE^2$"
      ],
      "metadata": {
        "id": "ECRQ9FTrTiKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = reg_model.mse_resid\n",
        "print('mse:', mse)\n",
        "\n",
        "#Calculating rse\n",
        "rse = np.sqrt(mse)\n",
        "print(\"rse:\", rse)"
      ],
      "metadata": {
        "id": "bPxZeEjCaMUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating RSE**\n",
        "**need to do a math example...**\n",
        "\n",
        "(1) Square each residual, take the sum of them.\n",
        "\n",
        "(2) calculate the degrees of freedom of the residuals. The number of observations - the nunber of model coefficients\n",
        "\n",
        "(3) take the square root of the ratio of these numbers. \n",
        "\n"
      ],
      "metadata": {
        "id": "dfsML3uPUjOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating RSE\n",
        "\n",
        "residuals_sq = reg_model.resid ** 2\n",
        "residual_sum_of_sq = sum(residuals_sq)\n",
        "deg_freedom = len(dataset.index) - 2\n",
        "rse = np.sqrt(resid_sum_of_sq/deg_freedom)\n",
        "\n",
        "print(\"resid sum of sq: \", resid_sum_of_sq)\n",
        "print(\"deg freedom: \", deg_freedom)\n",
        "print(\"rse:\", rse)"
      ],
      "metadata": {
        "id": "n7e3HcrFbQTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Root-mean-square error (RMSE) performs the same task as residual standard error, namely quantifying how inaccurate the model predictions are. "
      ],
      "metadata": {
        "id": "h5PQYIxyVkev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating RMSE\n",
        "# Important: RMSE does not account for the number of coefficients, worse for comparisions of models\n",
        "\n",
        "residuals_sq = reg_model.resid ** 2\n",
        "residual_sum_of_sq = sum(residuals_sq)\n",
        "n_obs = len(dataset.index)\n",
        "rmse = np.sqrt(resid_sum_of_sq/n_obs)\n",
        "\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "id": "TIoeneA9cmK6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}